{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('Claim Identifier', inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "train.drop('OIICS Nature of Injury Description', axis=1, inplace=True)\n",
    "\n",
    "#train.drop(columns=['Birth Year', 'Age at Injury', 'Number of Dependents', 'WCIO Cause of Injury Code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train.select_dtypes(include=np.number).columns.tolist()\n",
    "train_cat = train.select_dtypes(exclude=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns: Impute with mean\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "train[train_num] = pd.DataFrame(\n",
    "    num_imputer.fit_transform(train[train_num]),\n",
    "    columns=train_num,\n",
    "    index=train.index\n",
    ")\n",
    "\n",
    "# Categorical columns: Impute with most frequent (mode)\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "train[train_cat] = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(train[train_cat]),\n",
    "    columns=train_cat,\n",
    "    index=train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns='Claim Injury Type')\n",
    "y = train['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X.select_dtypes(include=np.number).set_index(X.index)\n",
    "X_cat = X.select_dtypes(exclude=np.number).set_index(X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "min_max.fit(X_num) #fit to training data\n",
    "X_num_scaled_min_max = min_max.transform(X_num) # this will return an array\n",
    "X_num_scaled_min_max = pd.DataFrame(X_num_scaled_min_max, columns = X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "min_max2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "min_max2.fit(X_num) #fit to training data\n",
    "X_num_scaled_min_max2 = min_max2.transform(X_num) # this will return an array\n",
    "X_num_scaled_min_max2 = pd.DataFrame(X_num_scaled_min_max2, columns = X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "# StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_num) # fit to training data\n",
    "X_num_scaled_standard = standard_scaler.transform(X_num) # this will return an array\n",
    "X_num_scaled_standard = pd.DataFrame(X_num_scaled_standard, columns=X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X_num) # fit to training data\n",
    "X_num_scaled_robust = robust_scaler.transform(X_num) # this will return an array\n",
    "X_num_scaled_robust = pd.DataFrame(X_num_scaled_robust, columns=X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X_cat.astype(str)\n",
    "\n",
    "enc1 = OrdinalEncoder() #encoder for features\n",
    "enc2 = LabelEncoder() #encoder for labels\n",
    "enc1.fit(X_cat)\n",
    "X_cat_encoded = pd.DataFrame(enc1.transform(X_cat), columns = X_cat.columns).set_index(X.index)\n",
    "y_encoded = enc2.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minmax = pd.concat([X_num_scaled_min_max, X_cat_encoded], axis=1)\n",
    "X_minmax2 = pd.concat([X_num_scaled_min_max2, X_cat_encoded], axis=1)\n",
    "X_standard = pd.concat([X_num_scaled_standard, X_cat_encoded], axis=1)\n",
    "X_robust = pd.concat([X_num_scaled_robust, X_cat_encoded], axis=1)\n",
    "y_encoded_df = pd.DataFrame(y_encoded, columns=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalers(data_scaled, names):\n",
    "    for i, name in zip(data_scaled, names):\n",
    "        print(name)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(i, y_encoded_df, test_size=0.2, stratify=y_encoded, random_state=5)\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        labels_train = model.predict(X_train)\n",
    "        labels_val = model.predict(X_val)\n",
    "        print(f1_score(y_train, labels_train, average='macro'))\n",
    "        print(f1_score(y_val, labels_val, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax\n",
      "1.0\n",
      "0.3789991801900998\n",
      "minmax2\n",
      "1.0\n",
      "0.3868077735332589\n",
      "standard\n",
      "1.0\n",
      "0.38678063703399973\n",
      "robust\n",
      "1.0\n",
      "0.3853890591724868\n"
     ]
    }
   ],
   "source": [
    "scalers([X_minmax, X_minmax2, X_standard, X_robust], ['minmax', 'minmax2', 'standard', 'robust'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
