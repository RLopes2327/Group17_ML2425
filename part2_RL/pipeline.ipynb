{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('Claim Identifier', inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "train.drop('OIICS Nature of Injury Description', axis=1, inplace=True)\n",
    "\n",
    "#train.drop(columns=['Birth Year', 'Age at Injury', 'Number of Dependents', 'WCIO Cause of Injury Code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train.select_dtypes(include=np.number).columns.tolist()\n",
    "train_cat = train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Numerical columns: Impute with mean\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "train[train_num] = pd.DataFrame(\n",
    "    num_imputer.fit_transform(train[train_num]),\n",
    "    columns=train_num,\n",
    "    index=train.index\n",
    ")\n",
    "\n",
    "# Categorical columns: Impute with most frequent (mode)\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "train[train_cat] = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(train[train_cat]),\n",
    "    columns=train_cat,\n",
    "    index=train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Claim Injury Type', axis=1)\n",
    "y = train['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X.select_dtypes(include=np.number).set_index(X.index)\n",
    "X_cat = X.select_dtypes(exclude=np.number).set_index(X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "min_max.fit(X_num) #fit to training data\n",
    "X_num_scaled_min_max = min_max.transform(X_num) # this will return an array\n",
    "X_num_scaled_min_max = pd.DataFrame(X_num_scaled_min_max, columns = X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "min_max2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "min_max2.fit(X_num) #fit to training data\n",
    "X_num_scaled_min_max2 = min_max2.transform(X_num) # this will return an array\n",
    "X_num_scaled_min_max2 = pd.DataFrame(X_num_scaled_min_max2, columns = X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "# StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_num) # fit to training data\n",
    "X_num_scaled_standard = standard_scaler.transform(X_num) # this will return an array\n",
    "X_num_scaled_standard = pd.DataFrame(X_num_scaled_standard, columns=X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X_num) # fit to training data\n",
    "X_num_scaled_robust = robust_scaler.transform(X_num) # this will return an array\n",
    "X_num_scaled_robust = pd.DataFrame(X_num_scaled_robust, columns=X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X_cat.astype(str)\n",
    "\n",
    "enc1 = OrdinalEncoder() #encoder for features\n",
    "enc2 = LabelEncoder() #encoder for labels\n",
    "enc1.fit(X_cat)\n",
    "X_cat_encoded = pd.DataFrame(enc1.transform(X_cat), columns = X_cat.columns).set_index(X.index)\n",
    "y_encoded = enc2.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minmax = pd.concat([X_num_scaled_min_max, X_cat_encoded], axis=1)\n",
    "X_minmax2 = pd.concat([X_num_scaled_min_max2, X_cat_encoded], axis=1)\n",
    "X_standard = pd.concat([X_num_scaled_standard, X_cat_encoded], axis=1)\n",
    "X_robust = pd.concat([X_num_scaled_robust, X_cat_encoded], axis=1)\n",
    "y_encoded_df = pd.DataFrame(y_encoded, columns=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X,y, model):\n",
    "    return model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X,y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    return f1_score(y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_f1_score(X,y,model, method=None):\n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    if isinstance(model, CategoricalNB) and method is not None: #CategoricalNB does not support kfold, we need to stratify\n",
    "        return\n",
    "    if method is None:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 0, \n",
    "                                                  stratify = y, \n",
    "                                                  shuffle = True)\n",
    "        model = run_model(X_train, y_train, model)\n",
    "        value_train = evaluate_model(X_train, y_train, model)\n",
    "        value_test = evaluate_model(X_test, y_test, model)\n",
    "        print('Train:', value_train)\n",
    "        print('Test:', value_test)\n",
    "    elif isinstance(method, StratifiedKFold):\n",
    "        for train_index, test_index in method.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            model = run_model(X_train, y_train, model)\n",
    "            value_train = evaluate_model(X_train, y_train, model)\n",
    "            value_test = evaluate_model(X_test, y_test, model)\n",
    "            score_train.append(value_train)\n",
    "            score_test.append(value_test)\n",
    "\n",
    "        print('Train:', np.mean(score_train))\n",
    "        print('Test:', np.mean(score_test))\n",
    "    else:\n",
    "        for train_index, test_index in method.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            model = run_model(X_train, y_train, model)\n",
    "            value_train = evaluate_model(X_train, y_train, model)\n",
    "            value_test = evaluate_model(X_test, y_test, model)\n",
    "            score_train.append(value_train)\n",
    "            score_test.append(value_test)\n",
    "\n",
    "        print('Train:', np.mean(score_train))\n",
    "        print('Test:', np.mean(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10) #if the splits are too many, poor efficiency\n",
    "rkf = RepeatedKFold(n_splits=6, n_repeats=2)  \n",
    "skf = StratifiedKFold(n_splits=10)  \n",
    "normal_split = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler:  minmax\n",
      "Model:  DecisionTreeClassifier()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.9999958208355446\n",
      "Test: 0.38330202502938454\n",
      "Method:  None\n",
      "Train: 1.0\n",
      "Test: 0.3815760263088144\n",
      "Model:  LogisticRegression()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.10398471934623876\n",
      "Test: 0.10397681538161746\n",
      "Method:  None\n",
      "Train: 0.10529113225531689\n",
      "Test: 0.10559662374073271\n",
      "Model:  GaussianNB(var_smoothing=0.0001)\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.18179852576293878\n",
      "Test: 0.18157608473218756\n",
      "Method:  None\n",
      "Train: 0.1825618165171516\n",
      "Test: 0.17991583799629762\n",
      "Model:  MLPClassifier()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.27024997767728587\n",
      "Test: 0.2695480323951222\n",
      "Method:  None\n",
      "Train: 0.24170397868077245\n",
      "Test: 0.2392474912379419\n",
      "Scaler:  standard\n",
      "Model:  DecisionTreeClassifier()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.9999958244581967\n",
      "Test: 0.38550177767078336\n",
      "Method:  None\n",
      "Train: 1.0\n",
      "Test: 0.3808351727801108\n",
      "Model:  LogisticRegression()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.10468356484353879\n",
      "Test: 0.10458530983695291\n",
      "Method:  None\n",
      "Train: 0.10373418413158014\n",
      "Test: 0.10403558680478721\n",
      "Model:  GaussianNB(var_smoothing=0.0001)\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.18187694085243591\n",
      "Test: 0.18202004405156166\n",
      "Method:  None\n",
      "Train: 0.18271515931297536\n",
      "Test: 0.18007665503256112\n",
      "Model:  MLPClassifier()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.2702813123010692\n",
      "Test: 0.26914529107868757\n",
      "Method:  None\n",
      "Train: 0.2601294179995715\n",
      "Test: 0.2584609248092874\n",
      "Scaler:  robust\n",
      "Model:  DecisionTreeClassifier()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.9999958169364017\n",
      "Test: 0.3836014968228236\n",
      "Method:  None\n",
      "Train: 1.0\n",
      "Test: 0.38436241585333525\n",
      "Model:  LogisticRegression()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.10381634767150033\n",
      "Test: 0.10374509613005539\n",
      "Method:  None\n",
      "Train: 0.10457549239783034\n",
      "Test: 0.10505755846700854\n",
      "Model:  GaussianNB(var_smoothing=0.0001)\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.18289272591429787\n",
      "Test: 0.18267897780421774\n",
      "Method:  None\n",
      "Train: 0.18414809038603736\n",
      "Test: 0.18154597377571793\n",
      "Model:  MLPClassifier()\n",
      "Method:  RepeatedKFold(n_repeats=2, n_splits=6, random_state=None)\n",
      "Train: 0.3233136264425864\n",
      "Test: 0.3227624621861089\n",
      "Method:  None\n",
      "Train: 0.33569634638170076\n",
      "Test: 0.33510582825074015\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "log = LogisticRegression()\n",
    "cnb = CategoricalNB()\n",
    "gnb = GaussianNB(var_smoothing=0.0001)\n",
    "knn = KNeighborsClassifier(algorithm='kd_tree', n_neighbors=19) #kd_tree is faster for large datasets and n_neighbors is the best value for this dataset\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "#start changing hyperparameters\n",
    "\n",
    "models = [dt, log, gnb, mlp] #knn\n",
    "scaler = ['minmax', 'standard', 'robust']\n",
    "data_scaled = [X_minmax, X_minmax2, X_standard, X_robust]\n",
    "for data, s in zip(data_scaled, scaler):\n",
    "    print(\"Scaler: \", s)\n",
    "    for model in models:\n",
    "        print(\"Model: \", model)\n",
    "        for method in [rkf, normal_split]:\n",
    "            print(\"Method: \", method)\n",
    "            avg_f1_score(data, y_encoded_df, model, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#não consigo usar o StratifiedKFold com o CategoricalNB, dá erro e não sei como resolver\n",
    "#imputing\n",
    "#scaling\n",
    "#encoding\n",
    "\n",
    "#Decision Tree overfits a lot, from 0.99 to 0.38\n",
    "#Logistic Regression is more stable, don't overfit but as poor results, from 0.10 to 0.10\n",
    "#using all columns or doing the feature selection doesn't change the results\n",
    "\n",
    "#starting using Kfold, repeated Kfold and normal split\n",
    "#no better results, Decision Tree still overfits a lot, Logistic Regression still poor results\n",
    "\n",
    "#adding CategoricalNB, GaussianNB and KNeighborsClassifier\n",
    "#CategoricalNB 0.32\n",
    "#GaussianNB 0.18\n",
    "#KNeighborsClassifier have just 0.23\n",
    "\n",
    "#imputing with KnnImputer takes too long, similar to using KNeighborsClassifier\n",
    "#maybe just using the imputer when it makes sense\n",
    "\n",
    "#final results:\n",
    "#the best is Decision Tree, 0.38\n",
    "#Logistic Regression is the worst, 0.10\n",
    "#CategoricalNB is 0.32 not so good as Decision Tree but don't overfit too much\n",
    "#GaussianNB is 0.18\n",
    "#MLPClassifier is 0.33\n",
    "\n",
    "#the best results is using Decision Tree with any scaler and any method\n",
    "#CategoricalNB and MPL Classifier with roubust scaler presents good results and don't overfit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
