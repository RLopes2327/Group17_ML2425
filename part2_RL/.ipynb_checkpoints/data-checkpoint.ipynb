{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('Claim Identifier', inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "train.drop('OIICS Nature of Injury Description', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WCIO Part Of Body Code'] = train['WCIO Part Of Body Code'].apply(lambda x: 0 if x < 0 else x)\n",
    "## IN DATE\n",
    "date_cols = ['Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date', 'First Hearing Date'] \n",
    "for col in date_cols:\n",
    "    train[col] = pd.to_datetime(train[col], errors='coerce')\n",
    "    test[col] = pd.to_datetime(test[col], errors='coerce')\n",
    "    \n",
    "# IN INT\n",
    "int_cols = ['Age at Injury', 'Birth Year', 'IME-4 Count', 'Number of Dependents']\n",
    "for col in int_cols:\n",
    "    train[col] = train[col].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset=['Claim Injury Type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['IME-4 Count'] = train['IME-4 Count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_to_object = ['Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "train[float_to_object] = train[float_to_object].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['Birth Year', 'Age at Injury', 'Number of Dependents', 'WCIO Cause of Injury Code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Alternative Dispute Resolution</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>C-3 Date</th>\n",
       "      <th>Carrier Name</th>\n",
       "      <th>Carrier Type</th>\n",
       "      <th>Claim Injury Type</th>\n",
       "      <th>County of Injury</th>\n",
       "      <th>COVID-19 Indicator</th>\n",
       "      <th>District Name</th>\n",
       "      <th>First Hearing Date</th>\n",
       "      <th>Gender</th>\n",
       "      <th>IME-4 Count</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>Industry Code Description</th>\n",
       "      <th>Medical Fee Region</th>\n",
       "      <th>WCIO Cause of Injury Description</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Description</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>WCIO Part Of Body Description</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Agreement Reached</th>\n",
       "      <th>WCB Decision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5393875</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>N</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>N</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NEW HAMPSHIRE INSURANCE CO</td>\n",
       "      <td>1A. PRIVATE</td>\n",
       "      <td>2. NON-COMP</td>\n",
       "      <td>ST. LAWRENCE</td>\n",
       "      <td>N</td>\n",
       "      <td>SYRACUSE</td>\n",
       "      <td>NaT</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>RETAIL TRADE</td>\n",
       "      <td>I</td>\n",
       "      <td>FROM LIQUID OR GREASE SPILLS</td>\n",
       "      <td>10.0</td>\n",
       "      <td>CONTUSION</td>\n",
       "      <td>62.0</td>\n",
       "      <td>BUTTOCKS</td>\n",
       "      <td>13662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not Work Related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393091</th>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>N</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Y</td>\n",
       "      <td>1745.93</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>ZURICH AMERICAN INSURANCE CO</td>\n",
       "      <td>1A. PRIVATE</td>\n",
       "      <td>4. TEMPORARY</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>N</td>\n",
       "      <td>ROCHESTER</td>\n",
       "      <td>2020-02-21</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CONSTRUCTION</td>\n",
       "      <td>I</td>\n",
       "      <td>REPETITIVE MOTION</td>\n",
       "      <td>49.0</td>\n",
       "      <td>SPRAIN OR TEAR</td>\n",
       "      <td>38.0</td>\n",
       "      <td>SHOULDER(S)</td>\n",
       "      <td>14569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not Work Related</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Accident Date Alternative Dispute Resolution Assembly Date  \\\n",
       "Claim Identifier                                                              \n",
       "5393875             2019-12-30                              N    2020-01-01   \n",
       "5393091             2019-08-30                              N    2020-01-01   \n",
       "\n",
       "                 Attorney/Representative  Average Weekly Wage   C-2 Date  \\\n",
       "Claim Identifier                                                           \n",
       "5393875                                N                 0.00 2019-12-31   \n",
       "5393091                                Y              1745.93 2020-01-01   \n",
       "\n",
       "                   C-3 Date                  Carrier Name Carrier Type  \\\n",
       "Claim Identifier                                                         \n",
       "5393875                 NaT    NEW HAMPSHIRE INSURANCE CO  1A. PRIVATE   \n",
       "5393091          2020-01-14  ZURICH AMERICAN INSURANCE CO  1A. PRIVATE   \n",
       "\n",
       "                 Claim Injury Type County of Injury COVID-19 Indicator  \\\n",
       "Claim Identifier                                                         \n",
       "5393875                2. NON-COMP     ST. LAWRENCE                  N   \n",
       "5393091               4. TEMPORARY          WYOMING                  N   \n",
       "\n",
       "                 District Name First Hearing Date Gender  IME-4 Count  \\\n",
       "Claim Identifier                                                        \n",
       "5393875               SYRACUSE                NaT      M            0   \n",
       "5393091              ROCHESTER         2020-02-21      F            4   \n",
       "\n",
       "                 Industry Code Industry Code Description Medical Fee Region  \\\n",
       "Claim Identifier                                                              \n",
       "5393875                   44.0              RETAIL TRADE                  I   \n",
       "5393091                   23.0              CONSTRUCTION                  I   \n",
       "\n",
       "                 WCIO Cause of Injury Description WCIO Nature of Injury Code  \\\n",
       "Claim Identifier                                                               \n",
       "5393875              FROM LIQUID OR GREASE SPILLS                       10.0   \n",
       "5393091                         REPETITIVE MOTION                       49.0   \n",
       "\n",
       "                 WCIO Nature of Injury Description WCIO Part Of Body Code  \\\n",
       "Claim Identifier                                                            \n",
       "5393875                                  CONTUSION                   62.0   \n",
       "5393091                             SPRAIN OR TEAR                   38.0   \n",
       "\n",
       "                 WCIO Part Of Body Description Zip Code  Agreement Reached  \\\n",
       "Claim Identifier                                                             \n",
       "5393875                               BUTTOCKS    13662                0.0   \n",
       "5393091                            SHOULDER(S)    14569                1.0   \n",
       "\n",
       "                      WCB Decision  \n",
       "Claim Identifier                    \n",
       "5393875           Not Work Related  \n",
       "5393091           Not Work Related  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=train.filter(like='Date').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Claim Injury Type', axis=1)\n",
    "y = train['Claim Injury Type']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10) #if the splits are too many, poor efficiency\n",
    "rkf = RepeatedKFold(n_splits=6, n_repeats=2)  \n",
    "skf = StratifiedKFold(n_splits=10)  \n",
    "normal_split = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, method=None):\n",
    "    if method is None:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, \n",
    "                                                random_state = 0, \n",
    "                                                stratify = y, \n",
    "                                                shuffle = True)\n",
    "    elif isinstance(method, StratifiedKFold):\n",
    "        for train_index, test_index in method.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    else:\n",
    "        for train_index, test_index in method.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    X_train_num = X_train.select_dtypes(include=np.number)\n",
    "    X_test_num = X_test.select_dtypes(include=np.number)\n",
    "    X_train_cat = X_train.select_dtypes(exclude=np.number)\n",
    "    X_test_cat = X_test.select_dtypes(exclude=np.number)\n",
    "\n",
    "    return X_train_num, X_test_num, X_train_cat, X_test_cat, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing(X_train_num, X_test_num, X_train_cat, X_test_cat):\n",
    "    # Numéricos: Imputação com média\n",
    "    num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_test_num = pd.DataFrame(num_imputer.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    # Categóricos: Imputação com moda\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    X_train_cat = pd.DataFrame(cat_imputer.fit_transform(X_train_cat), columns=X_train_cat.columns)\n",
    "    X_test_cat = pd.DataFrame(cat_imputer.transform(X_test_cat), columns=X_test_cat.columns)\n",
    "\n",
    "    return X_train_num, X_test_num, X_train_cat, X_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate IQR and identify outliers for a specific column\n",
    "def identify_outliers_iqr_column(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(X_train_num, y_train):\n",
    "    not_voluntary = X_train_num['Average Weekly Wage'] != 0\n",
    "    not_voluntary_df = X_train_num[not_voluntary]\n",
    "    \n",
    "    outliers_mask = identify_outliers_iqr_column(not_voluntary_df, 'Average Weekly Wage')\n",
    "    outliers_indices = not_voluntary_df[outliers_mask].index\n",
    "\n",
    "    X_train_num = X_train_num.drop(index=outliers_indices, errors='ignore')\n",
    "    y_train = y_train.drop(index=outliers_indices, errors='ignore')\n",
    "    \n",
    "    return X_train_num, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "min_max2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "standard_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X_train_num, X_test_num, scaler):\n",
    "    scaler.fit(X_train_num)  # Ajusta o escalonador aos dados de treino\n",
    "    X_train_num_scaled = pd.DataFrame(scaler.transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_test_num_scaled = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    return X_train_num_scaled, X_test_num_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot = OneHotEncoder()\n",
    "ordinal = OrdinalEncoder()\n",
    "label = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_independent(X_train_cat, X_test_cat, encoder):\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "    \n",
    "    encoder.fit(X_train_cat)\n",
    "    X_train_cat_encoded = pd.DataFrame(\n",
    "        encoder.transform(X_train_cat), columns=encoder.get_feature_names_out()\n",
    "    )\n",
    "    X_test_cat_encoded = pd.DataFrame(\n",
    "        encoder.transform(X_test_cat), columns=encoder.get_feature_names_out()\n",
    "    )\n",
    "\n",
    "    return X_train_cat_encoded, X_test_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_dependent(y_train, y_test, encoder):\n",
    "    encoder.fit(y_train)\n",
    "    y_train_encoded = pd.Series(encoder.transform(y_train))\n",
    "    y_test_encoded = pd.Series(encoder.transform(y_test))\n",
    "\n",
    "    return y_train_encoded, y_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X,y, model):\n",
    "    return model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X,y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    return f1_score(y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X, y, method, scaler, encoder_independent, encoder_dependent, model):\n",
    "    X_train_num, X_test_num, X_train_cat, X_test_cat, y_train, y_test = split_data(X, y, method)\n",
    "    print(\"Split data OK.\")\n",
    "    X_train_num, X_test_num, X_train_cat, X_test_cat = imputing(X_train_num, X_test_num, X_train_cat, X_test_cat)\n",
    "    print(\"Imputing OK.\")\n",
    "    X_train_num, y_train = outliers(X_train_num, y_train)\n",
    "    print(\"Outliers OK.\")\n",
    "    X_train_num_scaled, X_test_num_scaled = scaling(X_train_num, X_test_num, scaler)\n",
    "    print(\"Scaling OK.\")\n",
    "    X_train_cat_encoded, X_test_cat_encoded = encoding_independent(X_train_cat, X_test_cat, encoder_independent)\n",
    "    print(\"Encoding independent OK.\")\n",
    "    y_train_encoded, y_test_encoded = encoding_dependent(y_train, y_test, encoder_dependent)\n",
    "    print(\"Encoding dependent OK.\")\n",
    "\n",
    "    X_train = pd.concat([X_train_num_scaled, X_train_cat_encoded], axis=1)\n",
    "    X_test = pd.concat([X_test_num_scaled, X_test_cat_encoded], axis=1)\n",
    "    print(\"Concatenating OK.\")\n",
    "\n",
    "    model = run_model(X_train, y_train_encoded, model)\n",
    "    print(\"Model OK.\")\n",
    "    f1 = evaluate_model(X_test, y_test_encoded, model)\n",
    "    print(\"Evaluation OK.\")\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Split data OK.\n",
      "Imputing OK.\n",
      "Outliers OK.\n",
      "Scaling OK.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['TRAVELERS CASUALTY AND SURETY', 'NATIONAL FUEL GAS SUPP CORP', 'SECURITY INS CO OF HARTFORD', 'MID-HUDSON VALLEY STAFFCO, LL', 'BOCES DISTRICT OF ORANGE &', 'NATIONWIDE MUTUAL INSURANCE', 'PENN MILLERS INSURANCE CO', 'GREENPORT UFSD', 'GEORGIA PACIFIC CORPORATION', 'NATIONAL FUEL GAS DIST CORP', 'WHEATLAND CHILI CENTRAL', 'DUNDEE CENTRAL SCHOOL DIST', 'NISKAYUNA TOWN OF', 'KEENE CENTRAL SCHOOL DIST', 'KESHEQUA CSD', 'GENERAL BROWN CSD', 'MOUNTAIN VALLEY INDEMNITY CO', 'NATIONWIDE ASSURANCE COMPANY', 'POWER AUTHORITY OF THE STATE', 'HERMON-DEKALB CENTRAL', 'CHAZY CENTRAL SCHOOL DISTRICT', 'MIDWEST EMPLOYERS CAS. CO.', 'EDMESTON CENTRAL', 'GALWAY CENTRAL SCHOOL DIST', 'PHELPS-CLIFTON SPRINGS CENTRAL', 'MARATHON CENTRAL SCHOOL DIST', 'GREENE COUNTY', 'CAMBRIDGE CENTRAL SCHOOL', 'BROOKFIELD CENTRAL SCHOOL'] in column 2 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#0.99/0.39\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m pipeline(X, y, skf, robust_scaler, ordinal, label, dt)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[174], line 10\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(X, y, method, scaler, encoder_independent, encoder_dependent, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m X_train_num_scaled, X_test_num_scaled \u001b[38;5;241m=\u001b[39m scaling(X_train_num, X_test_num, scaler)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaling OK.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m X_train_cat_encoded, X_test_cat_encoded \u001b[38;5;241m=\u001b[39m encoding_independent(X_train_cat, X_test_cat, encoder_independent)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding independent OK.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m y_train_encoded, y_test_encoded \u001b[38;5;241m=\u001b[39m encoding_dependent(y_train, y_test, encoder_dependent)\n",
      "Cell \u001b[1;32mIn[170], line 10\u001b[0m, in \u001b[0;36mencoding_independent\u001b[1;34m(X_train_cat, X_test_cat, encoder)\u001b[0m\n\u001b[0;32m      5\u001b[0m encoder\u001b[38;5;241m.\u001b[39mfit(X_train_cat)\n\u001b[0;32m      6\u001b[0m X_train_cat_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m      7\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mtransform(X_train_cat), columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m X_test_cat_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m---> 10\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mtransform(X_test_cat), columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train_cat_encoded, X_test_cat_encoded\n",
      "File \u001b[1;32mc:\\Users\\lopes\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\lopes\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1578\u001b[0m, in \u001b[0;36mOrdinalEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;124;03mTransform X to ordinal codes.\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1578\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m   1579\u001b[0m     X,\n\u001b[0;32m   1580\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m   1581\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1582\u001b[0m     ignore_category_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_indices,\n\u001b[0;32m   1583\u001b[0m )\n\u001b[0;32m   1584\u001b[0m X_trans \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat_idx, missing_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_indices\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\lopes\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:214\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    210\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m during transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(diff, i)\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: Found unknown categories ['TRAVELERS CASUALTY AND SURETY', 'NATIONAL FUEL GAS SUPP CORP', 'SECURITY INS CO OF HARTFORD', 'MID-HUDSON VALLEY STAFFCO, LL', 'BOCES DISTRICT OF ORANGE &', 'NATIONWIDE MUTUAL INSURANCE', 'PENN MILLERS INSURANCE CO', 'GREENPORT UFSD', 'GEORGIA PACIFIC CORPORATION', 'NATIONAL FUEL GAS DIST CORP', 'WHEATLAND CHILI CENTRAL', 'DUNDEE CENTRAL SCHOOL DIST', 'NISKAYUNA TOWN OF', 'KEENE CENTRAL SCHOOL DIST', 'KESHEQUA CSD', 'GENERAL BROWN CSD', 'MOUNTAIN VALLEY INDEMNITY CO', 'NATIONWIDE ASSURANCE COMPANY', 'POWER AUTHORITY OF THE STATE', 'HERMON-DEKALB CENTRAL', 'CHAZY CENTRAL SCHOOL DISTRICT', 'MIDWEST EMPLOYERS CAS. CO.', 'EDMESTON CENTRAL', 'GALWAY CENTRAL SCHOOL DIST', 'PHELPS-CLIFTON SPRINGS CENTRAL', 'MARATHON CENTRAL SCHOOL DIST', 'GREENE COUNTY', 'CAMBRIDGE CENTRAL SCHOOL', 'BROOKFIELD CENTRAL SCHOOL'] in column 2 during transform"
     ]
    }
   ],
   "source": [
    "print('Decision Tree')#0.99/0.39\n",
    "dt = DecisionTreeClassifier()\n",
    "result = pipeline(X, y, skf, robust_scaler, ordinal, label, dt)\n",
    "print(f\"F1 Score: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" from xgboost import XGBClassifier #0.64/0.45\\nX_train, X_test, y_train, y_test = train_test_split(X_robust,y_encoded_df, test_size = 0.3, \\n                                                  random_state = 0, \\n                                                  stratify = y_encoded_df, \\n                                                  shuffle = True)\\nxg = XGBClassifier(\\n            n_estimators=250,\\n            learning_rate=0.1,\\n            max_depth=6,\\n            random_state=42,\\n            n_jobs=2,\\n            tree_method='hist',\\n            enable_categorical=True,\\n            objective='multi:softprob',\\n            num_class=8,\\n            eval_metric=['mlogloss', 'merror'],\\n            use_label_encoder=False\\n        )\\n\\n# Train with early stopping\\neval_set = [(X_train, y_train)]\\nxg.fit(\\n    X_train, y_train,\\n    eval_set=eval_set,\\n    verbose=False\\n)\\nvalue_train = evaluate_model(X_train, y_train, xg)\\nvalue_test = evaluate_model(X_test, y_test, xg)\\nprint('Train:', value_train)\\nprint('Test:', value_test) \""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from xgboost import XGBClassifier #0.64/0.45\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_robust,y_encoded_df, test_size = 0.3, \n",
    "                                                  random_state = 0, \n",
    "                                                  stratify = y_encoded_df, \n",
    "                                                  shuffle = True)\n",
    "xg = XGBClassifier(\n",
    "            n_estimators=250,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            random_state=42,\n",
    "            n_jobs=2,\n",
    "            tree_method='hist',\n",
    "            enable_categorical=True,\n",
    "            objective='multi:softprob',\n",
    "            num_class=8,\n",
    "            eval_metric=['mlogloss', 'merror'],\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "\n",
    "# Train with early stopping\n",
    "eval_set = [(X_train, y_train)]\n",
    "xg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")\n",
    "value_train = evaluate_model(X_train, y_train, xg)\n",
    "value_test = evaluate_model(X_test, y_test, xg)\n",
    "print('Train:', value_train)\n",
    "print('Test:', value_test) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" print('MLP')\\nmlp = MLPClassifier()\\navg_f1_score(X_robust, y_encoded_df, mlp, rkf) \""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print('MLP')\n",
    "mlp = MLPClassifier()\n",
    "avg_f1_score(X_robust, y_encoded_df, mlp, rkf) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m prob_cat_train \u001b[38;5;241m=\u001b[39m model_cat\u001b[38;5;241m.\u001b[39mpredict_proba(X_train_cat_encoded)\n\u001b[0;32m      8\u001b[0m prob_num_train \u001b[38;5;241m=\u001b[39m model_num\u001b[38;5;241m.\u001b[39mpredict_proba(X_train_num_scaled)\n\u001b[1;32m----> 9\u001b[0m prob_cat_test \u001b[38;5;241m=\u001b[39m model_cat\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_cat_encoded)\n\u001b[0;32m     10\u001b[0m prob_num_test \u001b[38;5;241m=\u001b[39m model_num\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_num_scaled)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Combinar probabilidades (média)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lopes\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:144\u001b[0m, in \u001b[0;36m_BaseNB.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Return probability estimates for the test vector X.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m        order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_log_proba(X))\n",
      "File \u001b[1;32mc:\\Users\\lopes\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:123\u001b[0m, in \u001b[0;36m_BaseNB.predict_log_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    121\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    122\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[1;32m--> 123\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# normalize by P(x) = P(f_1, ..., f_n)\u001b[39;00m\n\u001b[0;32m    125\u001b[0m log_prob_x \u001b[38;5;241m=\u001b[39m logsumexp(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lopes\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:1513\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_):\n\u001b[0;32m   1512\u001b[0m     indices \u001b[38;5;241m=\u001b[39m X[:, i]\n\u001b[1;32m-> 1513\u001b[0m     jll \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_log_prob_[i][:, indices]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   1514\u001b[0m total_ll \u001b[38;5;241m=\u001b[39m jll \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_log_prior_\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_ll\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 5"
     ]
    }
   ],
   "source": [
    "\"\"\" print('Naive Bayes') #0.37/0.32\n",
    "cnb = CategoricalNB(alpha= 0.1)\n",
    "gnb = GaussianNB(var_smoothing=1e-7)\n",
    "model_cat = run_model(X_train_cat_encoded, y_train_encoded, cnb)\n",
    "model_num = run_model(X_train_num_scaled, y_train_encoded, gnb)\n",
    "# Obter probabilidades de previsão\n",
    "prob_cat_train = model_cat.predict_proba(X_train_cat_encoded)\n",
    "prob_num_train = model_num.predict_proba(X_train_num_scaled)\n",
    "prob_cat_test = model_cat.predict_proba(X_test_cat_encoded)\n",
    "prob_num_test = model_num.predict_proba(X_test_num_scaled)\n",
    "# Combinar probabilidades (média)\n",
    "prob_combined_train = (prob_cat_train + prob_num_train) / 2\n",
    "prob_combined_test = (prob_cat_test + prob_num_test) / 2\n",
    "#prob_combined_train = (0.7 * prob_cat_train + 0.3 * prob_num_train)\n",
    "#prob_combined_test = (0.7 * prob_cat_test + 0.3 * prob_num_test)\n",
    "# Predizer classe final\n",
    "y_pred_combined_train = np.argmax(prob_combined_train, axis=1)\n",
    "y_pred_combined_test = np.argmax(prob_combined_test, axis=1)\n",
    "# Avaliar o modelo combinado\n",
    "print('Train:', f1_score(y_train_encoded, y_pred_combined_train, average='macro'))# y_test_num ou y_test_cat são os mesmos\n",
    "print('Test:', f1_score(y_test_encoded, y_pred_combined_test, average='macro')) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
