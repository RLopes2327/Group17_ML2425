{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('Claim Identifier', inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "train.drop('OIICS Nature of Injury Description', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WCIO Part Of Body Code'] = train['WCIO Part Of Body Code'].apply(lambda x: 0 if x < 0 else x)\n",
    "## IN DATE\n",
    "date_cols = ['Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date', 'First Hearing Date'] \n",
    "for col in date_cols:\n",
    "    train[col] = pd.to_datetime(train[col], errors='coerce')\n",
    "    test[col] = pd.to_datetime(test[col], errors='coerce')\n",
    "    \n",
    "# IN INT\n",
    "int_cols = ['Age at Injury', 'Birth Year', 'IME-4 Count', 'Number of Dependents']\n",
    "for col in int_cols:\n",
    "    train[col] = train[col].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset=['Claim Injury Type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['IME-4 Count'] = train['IME-4 Count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_to_object = ['Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "train[float_to_object] = train[float_to_object].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['Birth Year', 'Age at Injury', 'Number of Dependents', 'WCIO Cause of Injury Code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train.select_dtypes(include=np.number).columns.tolist()\n",
    "train_cat = train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Numerical columns: Impute with mean\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "train[train_num] = pd.DataFrame(\n",
    "    num_imputer.fit_transform(train[train_num]),\n",
    "    columns=train_num,\n",
    "    index=train.index\n",
    ")\n",
    "\n",
    "# Categorical columns: Impute with most frequent (mode)\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "train[train_cat] = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(train[train_cat]),\n",
    "    columns=train_cat,\n",
    "    index=train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate IQR and identify outliers for a specific column\n",
    "def identify_outliers_iqr_column(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_voluntary_train = train['Average Weekly Wage'] != 0\n",
    "\n",
    "# Identify outliers for the 'Average Weekly Wage' column\n",
    "outliers = identify_outliers_iqr_column(train[not_voluntary_train], 'Average Weekly Wage')\n",
    "train_cleaned = train[~(not_voluntary_train & outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_cleaned.drop('Claim Injury Type', axis=1)\n",
    "y = train_cleaned['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X.select_dtypes(include=np.number).set_index(X.index)\n",
    "X_cat = X.select_dtypes(exclude=np.number).set_index(X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "min_max.fit(X_num) #fit to training data\n",
    "X_num_scaled_min_max = min_max.transform(X_num) # this will return an array\n",
    "X_num_scaled_min_max = pd.DataFrame(X_num_scaled_min_max, columns = X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "min_max2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "min_max2.fit(X_num) #fit to training data\n",
    "X_num_scaled_min_max2 = min_max2.transform(X_num) # this will return an array\n",
    "X_num_scaled_min_max2 = pd.DataFrame(X_num_scaled_min_max2, columns = X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "# StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_num) # fit to training data\n",
    "X_num_scaled_standard = standard_scaler.transform(X_num) # this will return an array\n",
    "X_num_scaled_standard = pd.DataFrame(X_num_scaled_standard, columns=X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe\n",
    "\n",
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X_num) # fit to training data\n",
    "X_num_scaled_robust = robust_scaler.transform(X_num) # this will return an array\n",
    "X_num_scaled_robust = pd.DataFrame(X_num_scaled_robust, columns=X_num.columns).set_index(X.index) # Convert the array to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X_cat.astype(str)\n",
    "\n",
    "enc1 = OrdinalEncoder() #encoder for features\n",
    "enc2 = LabelEncoder() #encoder for labels\n",
    "enc1.fit(X_cat)\n",
    "X_cat_encoded = pd.DataFrame(enc1.transform(X_cat), columns = X_cat.columns).set_index(X.index)\n",
    "y_encoded = enc2.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_minmax = pd.concat([X_num_scaled_min_max, X_cat_encoded], axis=1)\n",
    "#X_minmax2 = pd.concat([X_num_scaled_min_max2, X_cat_encoded], axis=1)\n",
    "#X_standard = pd.concat([X_num_scaled_standard, X_cat_encoded], axis=1)\n",
    "X_robust = pd.concat([X_num_scaled_robust, X_cat_encoded], axis=1)\n",
    "y_encoded_df = pd.DataFrame(y_encoded, columns=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X,y, model):\n",
    "    return model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X,y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    return f1_score(y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_f1_score(X,y,model, method=None):\n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    if method is None:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 0, \n",
    "                                                  stratify = y, \n",
    "                                                  shuffle = True)\n",
    "        model = run_model(X_train, y_train, model)\n",
    "        value_train = evaluate_model(X_train, y_train, model)\n",
    "        value_test = evaluate_model(X_test, y_test, model)\n",
    "        print('Train:', value_train)\n",
    "        print('Test:', value_test)\n",
    "    elif isinstance(method, StratifiedKFold):\n",
    "        for train_index, test_index in method.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            model = run_model(X_train, y_train, model)\n",
    "            value_train = evaluate_model(X_train, y_train, model)\n",
    "            value_test = evaluate_model(X_test, y_test, model)\n",
    "            score_train.append(value_train)\n",
    "            score_test.append(value_test)\n",
    "\n",
    "        print('Train:', np.mean(score_train))\n",
    "        print('Test:', np.mean(score_test))\n",
    "    else:\n",
    "        for train_index, test_index in method.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            model = run_model(X_train, y_train, model)\n",
    "            value_train = evaluate_model(X_train, y_train, model)\n",
    "            value_test = evaluate_model(X_test, y_test, model)\n",
    "            score_train.append(value_train)\n",
    "            score_test.append(value_test)\n",
    "\n",
    "        print('Train:', np.mean(score_train))\n",
    "        print('Test:', np.mean(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10) #if the splits are too many, poor efficiency\n",
    "rkf = RepeatedKFold(n_splits=6, n_repeats=2)  \n",
    "skf = StratifiedKFold(n_splits=10)  \n",
    "normal_split = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Train: 0.9999743737183131\n",
      "Test: 0.3935725793952795\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree')#0.99/0.39\n",
    "dt = DecisionTreeClassifier()\n",
    "avg_f1_score(X_robust, y_encoded_df, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6468499867648003\n",
      "Test: 0.4588112705230834\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier #0.64/0.45\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_robust,y_encoded_df, test_size = 0.3, \n",
    "                                                  random_state = 0, \n",
    "                                                  stratify = y_encoded_df, \n",
    "                                                  shuffle = True)\n",
    "xg = XGBClassifier(\n",
    "            n_estimators=250,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            random_state=42,\n",
    "            n_jobs=2,\n",
    "            tree_method='hist',\n",
    "            enable_categorical=True,\n",
    "            objective='multi:softprob',\n",
    "            num_class=8,\n",
    "            eval_metric=['mlogloss', 'merror'],\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "\n",
    "# Train with early stopping\n",
    "eval_set = [(X_train, y_train)]\n",
    "xg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")\n",
    "value_train = evaluate_model(X_train, y_train, xg)\n",
    "value_test = evaluate_model(X_test, y_test, xg)\n",
    "print('Train:', value_train)\n",
    "print('Test:', value_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" print('MLP')\\nmlp = MLPClassifier()\\navg_f1_score(X_robust, y_encoded_df, mlp, rkf) \""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print('MLP')\n",
    "mlp = MLPClassifier()\n",
    "avg_f1_score(X_robust, y_encoded_df, mlp, rkf) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat_encoded,y_encoded_df, test_size = 0.3, \\n                                                  random_state = 0, \\n                                                  stratify = y_encoded_df, \\n                                                  shuffle = True)\\n\\nX_train_num, X_test_num, y_train_num, y_test_num = train_test_split(X_num_scaled_min_max,y_encoded_df, test_size = 0.3, \\n                                                  random_state = 0, \\n                                                  stratify = y_encoded_df, \\n                                                  shuffle = True) '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat_encoded,y_encoded_df, test_size = 0.3, \n",
    "                                                  random_state = 0, \n",
    "                                                  stratify = y_encoded_df, \n",
    "                                                  shuffle = True)\n",
    "\n",
    "X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(X_num_scaled_min_max,y_encoded_df, test_size = 0.3, \n",
    "                                                  random_state = 0, \n",
    "                                                  stratify = y_encoded_df, \n",
    "                                                  shuffle = True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" print('Naive Bayes') #0.37/0.30\\ncnb = CategoricalNB(alpha= 0.1)\\ngnb = GaussianNB(var_smoothing=1e-7)\\nmodel_cat = run_model(X_train_cat, y_train_cat, cnb)\\nmodel_num = run_model(X_train_num, y_train_num, gnb)\\n# Obter probabilidades de previsão\\nprob_cat_train = model_cat.predict_proba(X_train_cat)\\nprob_num_train = model_num.predict_proba(X_train_num)\\nprob_cat_test = model_cat.predict_proba(X_test_cat)\\nprob_num_test = model_num.predict_proba(X_test_num)\\n# Combinar probabilidades (média)\\nprob_combined_train = (prob_cat_train + prob_num_train) / 2\\nprob_combined_test = (prob_cat_test + prob_num_test) / 2\\n# Predizer classe final\\ny_pred_combined_train = np.argmax(prob_combined_train, axis=1)\\ny_pred_combined_test = np.argmax(prob_combined_test, axis=1)\\n# Avaliar o modelo combinado\\nprint('Train:', f1_score(y_train_num, y_pred_combined_train, average='macro'))# y_test_num ou y_test_cat são os mesmos\\nprint('Test:', f1_score(y_test_num, y_pred_combined_test, average='macro')) \""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print('Naive Bayes') #0.37/0.30\n",
    "cnb = CategoricalNB(alpha= 0.1)\n",
    "gnb = GaussianNB(var_smoothing=1e-7)\n",
    "model_cat = run_model(X_train_cat, y_train_cat, cnb)\n",
    "model_num = run_model(X_train_num, y_train_num, gnb)\n",
    "# Obter probabilidades de previsão\n",
    "prob_cat_train = model_cat.predict_proba(X_train_cat)\n",
    "prob_num_train = model_num.predict_proba(X_train_num)\n",
    "prob_cat_test = model_cat.predict_proba(X_test_cat)\n",
    "prob_num_test = model_num.predict_proba(X_test_num)\n",
    "# Combinar probabilidades (média)\n",
    "prob_combined_train = (prob_cat_train + prob_num_train) / 2\n",
    "prob_combined_test = (prob_cat_test + prob_num_test) / 2\n",
    "# Predizer classe final\n",
    "y_pred_combined_train = np.argmax(prob_combined_train, axis=1)\n",
    "y_pred_combined_test = np.argmax(prob_combined_test, axis=1)\n",
    "# Avaliar o modelo combinado\n",
    "print('Train:', f1_score(y_train_num, y_pred_combined_train, average='macro'))# y_test_num ou y_test_cat são os mesmos\n",
    "print('Test:', f1_score(y_test_num, y_pred_combined_test, average='macro')) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
